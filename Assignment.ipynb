{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install chromadb -q\n",
        "!pip install tiktoken -q\n",
        "!pip install pypdf -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "MltpIpd9uXDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"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\""
      ],
      "metadata": {
        "id": "0Coq8Rfaxg4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "llm = ChatOpenAI(temperature=1,model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "DA2HbLu-vLNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge loader\n",
        "Suported files : - pdf,txt,doc,xls,ppt"
      ],
      "metadata": {
        "id": "N5ki1s3c0zaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "pdf_loader = DirectoryLoader('./DATA/', glob=\"**/*.pdf\")\n",
        "txt_loader = DirectoryLoader('./DATA/', glob=\"**/*.txt\")\n",
        "word_loader = DirectoryLoader('./DATA/', glob=\"**/*.docx\")\n",
        "xls_loader= DirectoryLoader('./DATA/', glob=\"**/*.xls\")\n",
        "xlsx_loader= DirectoryLoader('./DATA/', glob=\"**/*.xlsx\")\n",
        "pptx_loader= DirectoryLoader('./DATA/', glob=\"**/*.pptx\")\n",
        "ppt_loader= DirectoryLoader('./DATA/', glob=\"**/*.ppt\")\n",
        "\n",
        "\n",
        "\n",
        "loaders = [pdf_loader, txt_loader, word_loader,xls_loader,xlsx_loader,pptx_loader,ppt_loader]\n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "print(f\"Documents number: {len(documents)}\")"
      ],
      "metadata": {
        "id": "ZfDlVyISx4kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking the Data"
      ],
      "metadata": {
        "id": "TB27F2R10b_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "b63SzRPvy1Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Embeding and Saving into VectorStore"
      ],
      "metadata": {
        "id": "rx0G6L-I0ic0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "jP2UNrLj0lui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain's QA chain and Chat History"
      ],
      "metadata": {
        "id": "ZcVOEn980MA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ],
      "metadata": {
        "id": "c0J2-4X0y19_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GARDIO UI INTEGRATION"
      ],
      "metadata": {
        "id": "UbtG01440BqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    chat_history = []\n",
        "\n",
        "    def user(query, chat_history):\n",
        "        print(\"User query:\", query)\n",
        "        print(\"Chat history:\", chat_history)\n",
        "\n",
        "\n",
        "        chat_history_tuples = []\n",
        "        for message in chat_history:\n",
        "            chat_history_tuples.append((message[0], message[1]))\n",
        "\n",
        "\n",
        "        result = qa({\"question\": query, \"chat_history\": chat_history_tuples})\n",
        "\n",
        "\n",
        "        chat_history.append((query, result[\"answer\"]))\n",
        "        print(\"Updated chat history:\", chat_history)\n",
        "\n",
        "        return gr.update(value=\"\"), chat_history\n",
        "\n",
        "\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "TwuSgfg1y4c2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}